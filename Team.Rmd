---
title: "Team"
author: "John Banta, Kelsey Mclnturff"
date: "10/31/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Github info

Owner username: jdbanta

Partner username: kmcintur

Repo: [ds202_lab5](https://github.com/jdbanta/ds202_lab5)

## Sections {.tabset}

### Processing the data

#### 1

```{r}
data<-read.table('diabetes.txt',header=TRUE)
head(data)
```

#### 2

```{r}
data[data==""]<-NA
data<-droplevels(data, "")
head(data)
```

#### 3

```{r}
vars<-names(data) %in% c("id", "bp.2s", "bp.2d")
diabetes_reduced<-data[!vars]
head(diabetes_reduced)
```

##### 4

```{r}
index.na=apply(is.na(diabetes_reduced), 1, any)
diabetes_clean<-diabetes_reduced[!index.na,]
```

#### 5

```{r}
dim(diabetes_clean)
```

This indicates that the dimensions of the new diabetes_clean dataset has 366 rows and 16 columns

### Exploring and transforming data

#### 6
```{r}
library(ggplot2)
ggplot(data = diabetes_clean ,aes(x = glyhb))+ geom_histogram(binwidth = .25) + 
          ggtitle("Histogram of Glycosolated Hemoglobin, Showing Right Skewness") +
          xlab("Glycosolated Hemoglobin")
```

The log of `glyhb` fixes the right skewness  by spreading out the clusters of values on the low value end of the histogram, but at the downfall that the histogram is less intuitive to read. Conclusions made about the transformed data can't be directly applied to the original data, so it is necessary to be mindful of this distinction. 


#### 7
```{r}
library(dplyr)

diabetes_clean = mutate(diabetes_clean, glyhb_star = log(glyhb))

ggplot(data = diabetes_clean, aes(x = glyhb_star))+ geom_histogram(binwidth = .25)+ 
          ggtitle("Histogram of Lof of Glycosolated Hemoglobin, Adjusted to Correct Skewness") +
          xlab("Log of Glycosolated Hemoglobin")
```

The histogram of `glyhb_star` is much closer to being symmetric.

#### 8
```{r}
diabetes_clean %>% group_by(frame) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(location) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(age) %>% summarise(mean.glyhb = mean(glyhb_star)) 

diabetes_clean %>% group_by(gender) %>% summarise(mean.glyhb = mean(glyhb_star)) 
```

The mean log of glyhb is highest for large frame and decreases as frame size does. The mean log of glyhb is slighlty higher in Buckingham than Louisa. Age seems to have some correlation with mean log of glyhb, with most of the highest values seen with higher ages. The highest mean log of glyhb is seen at age 85, and the lowest is seen at age 32. The mean log of glyhb is slightly higher for males than for females. These data suggest that an older man from Buckingham with a large frame would have the highest log of glyhb on average.


### Visualizations

#### 10
```{r}
diabetes_clean$frame <- ordered(diabetes_clean$frame, levels = c("small", "medium", "large"))

diabetes_clean %>% group_by(frame,location) %>% summarise (mean.glyhb_star= mean(glyhb_star)) %>%
    ggplot( aes(x = frame, y = mean.glyhb_star, color=location)) +
    geom_point() +
    ggtitle("Scatterplot of Mean Log of Glycosolated Hemoglobin by Frame Size, Colored by Location") +
          ylab("Mean Log of Glycosolated Hemoglobin") +
          xlab("Frame Size")
```

#### 11
```{r}
fit = lm(glyhb_star~ratio+bp.1s+age+gender+hip+weight,data=diabetes_clean)
summary(fit)
```


#### 12
```{r}
library(hexbin)

ggplot(data = diabetes_clean, aes(x=waist, y=hip)) + geom_hex() + 
    facet_wrap(~frame) + 
    scale_fill_continuous(high = "#132B43", low = "#56B1F7") +
    ggtitle("Waist and Hip Patterns, Faceted by Frame Size")

```

Using `geom_hex` is one option to intuitively display the density of points where overplotting occurs.

```{r}
ggplot(diabetes_clean,aes(y=hip,x=waist)) + geom_point(shape = ".") + facet_wrap(~frame) +
        geom_smooth(method=lm) +
        ggtitle("Waist vs Hip Scatter Plot, Faceted by Frame Size")
```

Another way to circumvent the loss of information due to overplotting is to make the points very small, so they can be visible when very close to other points. I have also added a linear trend line to the data to make the distict pattern more clear.


### Messy data

#### 13

`spread` takes pairs of keys and values that are contained in different columns and arranges them in a way that can be more useful, dependending on the data. One column of keys will become the new column headers, while the value paired with the key will be listed beneath it, without dropping any other variables tied to the values.

`gather` does the opposite of `spread`. It takes column names and gathers them into a single key column and collects the values below the column name and puts them into a value column. 


#### 14

Yes, `spread` and `gather` are exact complements of each other. You can use `spread` to exactly undo `gather` and vice versa. This is because the key/value pairs are always maintained between the two. The order of the columns may be altered though, depending on the structure of the original data frame.

### Regression models

#### 15

```{r}
fit = lm(glyhb ~stab.glu + age + waist + ratio+ factor(frame),data=diabetes_clean)
 summary(fit)
```

Looking first at the summary output, the F-statistic is 77.49 with an associated p-value of <.001 indicates that this model is useful for predicting the Glycosolated Hemoglobin with the combination of the input variables. Furthermore the adjusted R-squared is .557, which indicates that the model, after being adjusted for its complexity can still explain 55.7% of the variability in the response. However, it appears that the final two variables are not significant predictors of the Glycosolated Hemoglobin, so we could probably remove them to help simplify the model and avoid overfitting. 

#### 16

The estimate of stab.glu indicates that for every unit increase in stab.glu, with all other variables held constant, then the Glycosolated Hemoglobin will increase by .0035182. Similarly with the age, waist and ratio variables with increases of .0033632, .0047925, and .0219341 respectively. With the factor variables an individual with a medium frame is predicted to have .0309167 more Glycosolated Hemoglobin than the baseline of the large frame. Similarly with the small frame the model predicts .0131840 more than the baseline of the large frame. 

#### 17

```{r}
fit$fitted.values
```

These fitted values are predictions of Glycosolated Hemoglobin using the information of the given rows to predict it.

```{r}
pred<-data.frame(stab.glu = 90, age = 35, waist = 30, ratio = 5.1,frame = "small")
predict(fit,pred)
```

For someone with a stab.glu=90, age=35, waist=30, ratio=5.1 and a small frame the model predicts a value of 4.870237

#### 18

Inference is using a model to learn about the model generated by your data. However prediction is taking what you learned from the data and use it to predict for a datapoint that isn't in the dataset. 

#### 19

Linear regression will be only impacted by the inputs that go into the equation, however the knn regression model can be impacted by the amount of neighbors that you look at. The nice thing about knn regression is that it will be put together with data that are very similar, however with linear regression one variable change could drastically change the output.

### Reflection

#### 20

  Our original idea of data science, which is similar to the Hal Varian quote in the beginning of lecture 1, was the ability to extract and communicate meaning from data. This course has given us practice at doing so using R, and our views have not changed. What was most surprising about data science is the collaborative community that is available for help on the internet. It is remarkable how much you can learn from Googling questions and errors. It seems possible to learn anything you are trying to accomplish by simply searching for it. What was most challenging was exploring data and knowing what sort of things to look for. Large datasets seem like a daunting spread of meaningless numbers until you get deeper into exploration. What was most enjoyable was creating good-looking and meaningful visuals from those originally daunting datasets. 